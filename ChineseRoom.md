#	Can the Chinese Room Argument really refute Strong Artificial Intelligence?

In 1980, American philosopher John Searle proposed the Chinese Room thought experiment, but similar ideas can be traced back to the famous 18th-century mathematician and philosopher Gottfried Leibniz, who proposed calculus in mathematics and monadology in philosophy.

The thought experiment can be described as follows:

Suppose a person who does not understand Chinese at all, such as Searle himself, is locked in a closed room. The room has two small windows, one for receiving Chinese information input from the outside world, and the other for outputting Chinese information to the outside world. There are three things in the room:

First, a set of Chinese characters, equivalent to data in a computer program; second, a set of English-written rule books, instructing how to process these Chinese characters and associating some characters with others, equivalent to a computer program; third, some draft paper, pencils and other stationery for executing the instructions in the manuals.

People outside the room constantly feed questions written in Chinese into the room. Although the person inside does not understand Chinese, he can follow the instructions in the manual to compare, match and combine the received Chinese characters with the existing Chinese characters in the room, and generate new strings of Chinese characters as output according to the instructions, which are handed out of the room.

From the outsider's perspective, the room seems to be able to communicate in Chinese, and the quality of the responses is indistinguishable from that of a real native Chinese speaker, i.e., there are no grammatical or semantic errors. But the person inside the room still does not understand Chinese at all, he is only mechanically executing the rules of symbol manipulation without understanding the meaning of the symbols.

Using this thought experiment, Searle "proved" that strong artificial intelligence, i.e., artificial intelligence that truly understands semantics, is impossible, and at most only weak artificial intelligence can be achieved, i.e., simulating human behavior without truly understanding.

However, are things really as rosy as Searle envisioned?

No.

Let's first simplify the Chinese room problem, which actually argues the following points:

-	First, we agree that in a room there is a person who does not understand Chinese, a complete Chinese rule book, and some auxiliary equipment;
-	Second, for any Chinese content input into the room, the person in the room can provide a grammatically correct and semantically appropriate response according to the rulebook;
-	Finally, through the content input to the room and feedback output from the room, outsiders can only conclude that the respondent understands Chinese, and cannot conclude that the respondent does not understand Chinese.

The problem is that according to the agreement, the person in the room should not understand Chinese, but in the third agreement, the conclusion is reached that the respondent understands Chinese, indicating that the person in the room is only simulating the behavior of someone who understands Chinese without truly understanding Chinese. Since the test represented by the third agreement is essentially a Turing test, Searle argues the flaw of the Turing test: it can only determine whether an object is a weak AI, but not whether an object is a strong AI, where weak AI refers to not understanding but only imitating human behavior, while strong AI indicates understanding and making appropriate behavioral feedback.

At first glance, this chain of reasoning seems reasonable, but if we analyze it in depth, we will find that there are actually many problems here.

The first is the holistic refutation. In this refutation, we consider the person in the room, the rulebook, the auxiliary equipment, and even the room itself as a whole to be the "respondent", and whether the person in the room as a part of the whole understands Chinese or not does not affect whether the room as a whole understands Chinese or not. This is what we often see in complex systems and emergent phenomena, where the attributes and capabilities of the parts that make up the whole are not necessarily related to the attributes and capabilities of the whole. This is also very intuitive from the perspective of algorithmic information theory. Interested friends can watch our previous episodes, so we won't go into details here.

For the holistic refutation, Searle himself also gave a refutation: we let the person in the room memorize the entire rulebook, and then walk out of the room. You can write Chinese on a piece of paper and show it to him, and he will process it according to the rulebook he has memorized, completing all the intermediate computational process in his mind, and then write the result on the paper. In this way, you can in no way conclude that the person who has now walked out of the room does not understand Chinese, but it clearly still does not understand Chinese.

This refutation to the holistic refutation looks very powerful, but in fact there are a few problems:

First, why can't we say that the person in the room now actually understands Chinese?

Second, how do we know that the person talking to me now is not essentially such a person in the room, so in fact he does not know what we are talking about at all, and is only simulating normal human responses based on the rules in his head?

The second question actually reflects the philosophical zombie problem: if an object cannot be judged to be human based on external behavior, then how can you be sure that a human is truly human? This problem can not only argue that strong AI is impossible, but in fact can also argue that humans are impossible, i.e.: **Humans can never truly be human, humans can only be human-like**.

Therefore, the second question actually denies the humanity of humans themselves, which is very ridiculous.

Let's return to the first question: Why can't we say that the person in the room who has now walked out actually understands Chinese?

Regarding this question, Searle's reply is: The person in the room, based on the rulebook he has memorized, can only be said to have remembered the grammatical rules of Chinese, but cannot establish an effective connection between Chinese characters and real semantics. In other words, the current person in the room can at most be said to have understood Chinese grammar, but does not understand Chinese semantics.

But this statement itself is problematic, because according to the second agreement in the Chinese room problem, i.e., the person in the room must provide a grammatically correct and semantically appropriate response, in order to achieve the third agreement, i.e., outsiders can only conclude from the response of the person in the room that he understands Chinese. But if the person in the room now only remembers Chinese grammar but does not understand Chinese semantics, then how is the second agreement achieved?

Here we either have to believe that semantic appropriateness or correctness depends only on grammatical correctness and not on understanding semantics, otherwise the dual agreement of giving semantically appropriate or correct responses without understanding semantics cannot be reached; or, we can only believe that if the person in the room only remembers Chinese grammar but does not understand Chinese semantics, he cannot give semantically correct or appropriate responses, thus contradicting the second agreement in the original problem. Or, as long as the second agreement can be reached, then the current person in the room, i.e., the whole of the person in the room, the rulebook, the auxiliary equipment and even the room in the original problem, can understand Chinese semantically, so Searle's refutation is invalid.

And, according to Tarski's formal linguistics, we know that the first case is impossible at the formal language level, i.e., the set of grammatically correct sentences cannot uniquely determine the semantics of the language used, so grammatically correct responses cannot guarantee semantic appropriateness or correctness. Since we cannot achieve "semantic correctness depends only on grammar and not on semantics" at the formal language level, it is even more impossible for ordinary languages with complexity far exceeding formal languages, so the first of the above three situations does not hold, leaving us with only two paths:

-	Either, we agree with Searle's refutation, so that the second agreement in the original problem, i.e., the Chinese room can give both grammatically and semantically correct responses, is impossible to achieve, so the Chinese room problem itself does not hold;
-	Or, we believe that the Chinese room problem itself holds, so that the second agreement holds, and thus Searle's refutation is invalid.

In other words, **Searle's refutation of the holistic refutation of the Chinese room problem, and the Chinese room problem proposed by Searle, cannot both be valid at the same time, one of them must be invalid.**

So, which one is invalid?

If we believe that the Chinese room problem itself is valid, i.e., we can really only use the person in the room and the rulebook, plus some auxiliary equipment, to make the room as a whole respond to any Chinese content in a grammatically and semantically correct way, and make the person outside conclude that the person inside really understands Chinese. Then that means Searle's refutation of holism is itself invalid, and this invalidity is reflected in this point: the person in the room who can achieve the above second agreement actually already understands Chinese.

But such a conclusion seems to have some obvious deviations from our intuitive notions, i.e., the person in the room obviously has not learned Chinese, he has only memorized the rulebook and can perform complex operations and output according to the rules and Chinese input. In other words, how can a person who has not systematically learned Chinese learn Chinese by simply memorizing rules?

Here there are a series of sub-questions:

-	First, is the rulebook only grammatical rules? Or is a rulebook containing only grammatical rules sufficient for the person in the room to achieve the second agreement?
-	Second, is it possible that the person in the room can proficiently handle a bunch of symbols, while not thinking that he is processing Chinese, the processing is equivalent to Chinese processing?
-	Finally, is the above description merely a linguistic deception? That is, merely through clever selection of words, creating a statement that the person in the room has not learned Chinese, which is not a fact?

Let's look at the third question first.

Regarding whether the person in the room has actually "learned Chinese", we have always maintained in the statement of the problem that the person in the room has not systematically learned Chinese, but is this really the case?

The core issue here is: what is learning?

Or, can effective learning be achieved through self-study and interaction with the environment?

After all, the discourse on whether the person outside the room can acquire Chinese and whether the person inside the room has not acquired Chinese comes entirely from the one-sided words of those discussing the question, and we have not given a determination of whether the person inside the room has really learned Chinese. In this case, we cannot rule out the possibility that the person inside the room has truly learned Chinese by memorizing the rules and processing the Chinese characters on the slips of paper passed into the room.

Of course, from the connectionist point of view held by Searle, such learning is not semantic learning, because obviously for words like "red", the person in the room only sees these symbols and has never seen their connection with the real "red", so it is impossible to semantically understand this word, and naturally it is also impossible to truly semantically understand more other Chinese words.

Let's suspend this question for now and continue later.

What needs to be pointed out here is that when people discuss issues related to artificial intelligence, they often make a conscious or unconscious choice to use vocabulary that can highlight human superiority on the human side, and use vocabulary that can emphasize non-human inferiority on the non-human side.

For example, when humans are thinking about which word to choose in a sentence, the most common example is whether the word "nao" (disturb) in "qingfeng mingyue nao wo xin" (the clear breeze and bright moon disturb my heart) can be replaced by other words, such as "luan" (mess up), "rao" (disturb), "liao" (tease), etc. We will use some relatively beautiful positive words in this process, such as "deliberate" on what word to use, "analyze" what word to use, and so on. We will emphasize in our choice of words that the actual selection of words is a process of using our heart, invoking our sensibility and rationality, analyzing every aspect of each word, and then choosing the most appropriate one.

However, if the same process is done by an artificial intelligence, the result is completely different. We will say that the artificial intelligence is "calculating" which word has the highest probability. You see, the same process, when it comes to artificial intelligence, becomes a cold calculation, with no sense of heart at all.

But the actual situation is that, at least for now, we cannot make a judgment on whether there is an essential difference between the two subjects when doing the same behavior, because we do not know what the actual thinking process of humans is. We may know our own thinking process, but this thinking process is presented by the brain to the consciousness, not the real operating process of the brain. Putting it on the artificial intelligence side, what we consider to be our own thinking process is nothing more than a text record output by the artificial intelligence in an internal log. Every programmer knows that I can write such a text record however I want, and we cannot conclude that the artificial intelligence is really deliberating on word choice just because the log writes about the analysis process of "rao", "nao", and "liao". Similarly, we cannot conclude that the brain cells of a poet are actually poets rather than neurons just because such an analysis process appears in the poet's mind log.

Therefore, returning to the third question, we currently do not have a strict determination of whether an object has acquired Chinese. Under this premise, directly saying that the outsider can acquire Chinese while the insider has not acquired Chinese is at least insufficiently argued and cannot be considered a credible argument, but can only be considered a priori position of a certain party, nothing more.

Next, let's consider the second question: Can the process of the person in the room processing Chinese input according to the requirements of the rulebook be equivalent to a process of understanding Chinese?

Or, let us rephrase this question:

Is it possible for the person in the room to be in such a state during the above processing that both the person in the room and the person outside the room believe that he is processing something other than Chinese, while the entire processing is actually completely isomorphic or equivalent to a real and effective Chinese processing?

Here, isomorphism means that the two processing processes can be completely equated under some mapping, while equivalence means that although the two processing processes seem to be different, they are actually the same. For example, in quantum mechanics, there are wave mechanics, matrix mechanics, and path integral mechanics, which at first glance seem to be different from each other, but can be proven to be completely equivalent to each other.

Returning to the Chinese processing problem, let's make a permutation of the Chinese characters, replacing "wo" with "Naruto", "ni" with "Sasuke", and so on, with each Chinese word or phrase replaced by a completely different word or phrase. So what the person in the room does according to the rulebook is to first replace the normal Chinese input with the above exotic Chinese, then process this exotic Chinese according to a set of rules that we normal Chinese users cannot understand at all, and then replace this exotic Chinese with normal Chinese again before outputting it to the person outside the room, and everything looks normal to the person outside.

In this process, if we only look at the intermediate processing of exotic Chinese, we will conclude that the person in the room actually does not understand Chinese at all, but in fact, his processing of Chinese is normal and correct. These two sets of processing are actually completely isomorphic.

In addition to permutation, there are many different methods to construct equivalent or even isomorphic processing that look completely different at first glance. This is just like solving mathematical problems. Sometimes we can have multiple solutions from different domains. Can we say that one method is understanding mathematics, while the other methods are not understanding mathematics? Obviously not.

Therefore, the question here is: **Can we judge that the object performing the processing actually does not understand Chinese because the intermediate processing is different from the processing we are familiar with?**

**The answer is: no.**

In this way, how can we judge that the Chinese processing performed by the person in the room according to the rulebook and auxiliary equipment is not a seemingly different but essentially isomorphic or equivalent Chinese processing? If it can be considered an isomorphic or equivalent Chinese processing, then it actually shows that the person in the room has understood and mastered the correct processing of Chinese through the rulebook and auxiliary equipment - in an isomorphic or equivalent sense. Therefore, we can say that it has actually acquired Chinese, except that the referent and signified of semantics here differ from those of the person outside the room by a mapping when using the same Chinese.

This can lead to another common speculative question about artificial intelligence, the Mary's room problem: A girl named Mary lives in a room with only black and white colors, and she has learned all the knowledge about color, including physics, biology, art, literature and other fields related to color. She even knows through learning this knowledge how to match colors to paint the most beautiful and realistic paintings, but she has only ever seen the two colors of black and white. Does she really understand red? Does she acquire new knowledge about red after leaving this black and white room and seeing real red?

This is actually the question we suspended earlier when discussing the third question: From the connectionist point of view that Searle believes in, the person in the room cannot establish an effective connection between the "red" in the text and the real "red", he is the girl Mary, who has acquired a lot of textual knowledge about red, but has never experienced real red, so this acquisition of knowledge does not mean real semantic understanding.

But this statement itself is still problematic.

Let's first look at Mary's room problem. The problem itself is actually wrong here, because it presupposes that a person can master all "direct experience" by "mastering all indirect experience", because only in this way can the premise of "acquiring all knowledge about red" be achieved, otherwise the premise is not met and the problem itself cannot be discussed. And this presupposition itself is obviously untenable, so this problem itself does not hold.

Let's back to the Chinese room problem, Searle's connectionist challenge essentially reflects how to misappropriate the concept of the term "understanding Chinese". In other words, **Searle is actually replacing "understanding Chinese" with "understanding the referents of all terms"**. Because he argues that the person in the room does not understand Chinese based on the fact that they do not understand the referent of a certain term in Chinese, such as "red". But the question is, does understanding Chinese really require understanding the referents of every term in Chinese?

For example, **suppose there is a blind person discussing the issue of red with you, and all communication is grammatically and semantically flawless, but this blind person has never seen red with their own eyes. Can you say they don't understand Chinese because of this?**

No, you can't.

This blind person certainly understands Chinese and knows Chinese. What they don't understand or know is merely the term called "red" in Chinese.

A blind person cannot truly understand the referents of words like "red", "flower", "cloud", and "sky", but that does not mean a blind person cannot truly understand Chinese.

Similarly, it is almost indisputable that the person in the room cannot understand terms like "red", "flower", "cloud", and "sky" through rule books and such. But **we cannot therefore conclude that the person in the room cannot understand Chinese, unless Searle believes that all humans with missing senses cannot truly understand any language used by humans**. And if we really insist on this ridiculous belief, then we know that a small number of humans have four types of photoreceptor cells in their eyes instead of the normal three types. So in the eyes of these people, all normal people lack direct experience of the fourth primary color. Then all normal humans cannot truly understand the world they see. According to Searle's connectionist thinking, we would have to draw the utterly ridiculous conclusion that all normal humans cannot truly understand any language.

Therefore, **between the idea that no normal human can understand any language, and the idea that the person in the room can understand Chinese even though they cannot understand the referents of all terms in Chinese, which do you think is more reasonable and normal?**

So, regarding whether it is possible for the person in the room to acquire Chinese through rule books and such, there are only two things we can clearly and justifiably say at present:

1.	Not being able to truly understand the referents of terms in a language does not mean not being able to truly understand that language;
2.	The process of processing language appearing different from how humans process language also cannot prove that the subject has not truly understood that language.

Beyond this, everything else is a matter of belief until we have a physical object to conduct further experimental verification for us. Various theoretical explanations can be given, but none can be substantive proof or falsification.

Finally, the first question is what kind of rules exactly are in the rule book in the Chinese room in order to ensure that the person in the room can achieve Convention 2?

This is a very open question, because we have not yet been able to give a clear conclusion on this issue in linguistics so far.

But at least we can confirm what this set of rules is not.

First of all, it is obvious that this rule book must be finite, otherwise the person in the room would not be able to complete the conversation in a finite amount of time, and the person outside the room could then determine that the person in the room does not understand Chinese.

Therefore, the rule book cannot possibly be a set of mappings constituting the output for every possible input, because there are countably infinite possible inputs, so such a set of mappings must also be countably infinite.

The rule book also obviously cannot be a set of fixed, hard-coded rules, because we have almost confirmed in real-life translation problems that such fixed rules are impossible to succeed. More precisely, if the rules are hard-coded, then for the same input, it can only give the same answer. Such a response does not conform to human communication habits and can be easily verified by the person outside the room as not understanding Chinese at all.

More rigorously speaking, the rules in the rule book must not only be a function of the current input, but also a function of the conversation history; at the same time, its output cannot be unique, but a distribution over multiple possible outputs. Therefore, it is at least a first-order Markov process, and the output is not a single sentence, but a distribution function over multiple sentences.

Of course, this alone is still not enough. This probabilistic Markov process must also be a function of the mental state, i.e., it must not only consider the current input and the entire conversation history, but also the current mental state of the interlocutor, and even the history of changes in the mental state.

At this point, it all seems like a Turing machine, or more precisely, a non-deterministic Turing machine, except that this Turing machine can be very complex, making it completely incomprehensible how it works and what the purpose of each step is.

So, is it possible to go beyond the Turing machine?

Gödel and others believed that human thought processes must necessarily include some processes or elements that transcend Turing machines, and the sources of these beliefs are all inseparable from Gödel's incompleteness theorems. We will further analyze this point in a future episode of the "Brain Twister Live" program.

At least, so far no one has been able to propose a truly effective thinking model that transcends Turing machines. Scientists have not been able to propose it, and neither have philosophers. Everyone believes there will be one, but no one has ever found it.

This question is just like what is consciousness, what is free will, and other such ultimate questions. Humans have been pursuing them for several thousand years, but have never been able to find the answer to the question.

All we can say at this point is that the Chinese room problem cannot truly refute the possibility of strong artificial intelligence. At least arguing against the possibility of strong artificial intelligence solely from the Chinese room problem is very inadequate - of course, we also cannot therefore conclude that strong artificial intelligence will definitely be realized. At least, there is also no sufficient argument between whether it can be realized and the currently popular LLMs.